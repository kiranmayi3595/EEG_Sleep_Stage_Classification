# -*- coding: utf-8 -*-
"""Sleep_Stage_Classification_CNN_U-Sleep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LECrluymfyWFb7XrsmXoHwb-mAWPekkl

## Sleep Stage Classification from PSG Data using U-Sleep CNN
This project performs a supervised multiclass classification of sleep stages using EEG data from the Sleep Physionet dataset. We aim to predict the sleep stages of one subject (Drake) using a deep learning model trained on another subject (Candice), following a sequence-to-sequence approach.

Each 30-second segment of EEG data is labeled as one of five standard sleep stages:

W ‚Äì Wake

N1 ‚Äì Light Sleep

N2 ‚Äì Intermediate Sleep

N3/4 ‚Äì Deep Sleep

REM ‚Äì Rapid Eye Movement Sleep

We preprocess the raw EEG signals, create overlapping sequences of 30-second windows, and train a U-Sleep convolutional neural network (CNN) model using the Braindecode library. The model learns to leverage temporal context by classifying multiple windows at once.

We then evaluate the model‚Äôs performance on Drake‚Äôs full-night EEG recording to assess how well this architecture generalizes across individuals.

üîß This tutorial is adapted from the official Braindecode U-Sleep example [1](https://braindecode.org/stable/auto_examples/applied_examples/plot_sleep_staging_usleep.html).
"""

# @title
!pip install braindecode

"""# Load the Data"""

# @title
from braindecode.datasets import SleepPhysionet
subject_ids=[2,3]
dataset = SleepPhysionet(subject_ids=subject_ids,recording_ids=[2], crop_wake_mins=30, crop=None)

"""# Preprocess Raw Data
Deep Learning models work on raw or minimally processed data. They are
1.   Sensitive to scale of inputs
2.   require data normalization across channels or recordings

"""

from braindecode.preprocessing import preprocess, Preprocessor
from sklearn.preprocessing import robust_scale #robust_scale removes effects of outliers ((signal-median)/IQR)

preprocessors = [Preprocessor(robust_scale, channel_wise=True)]

# Transform the data
preprocess(dataset, preprocessors)

"""# Extract 30s windows"""

#create_windows_from_events is a function that helps split raw EEG into labelled windows using annotations from the hypnogram
from braindecode.preprocessing import create_windows_from_events

mapping = {  # We merge stages 3 and 4 following AASM standards.
    'Sleep stage W': 0,
    'Sleep stage 1': 1,
    'Sleep stage 2': 2,
    'Sleep stage 3': 3,
    'Sleep stage 4': 3,
    'Sleep stage R': 4,
}

window_size_s = 30
sfreq = 100
window_size_samples = window_size_s * sfreq

windows_dataset = create_windows_from_events(
    dataset,
    trial_start_offset_samples=0,
    trial_stop_offset_samples=0,
    window_size_samples=window_size_samples,
    window_stride_samples=window_size_samples,
    preload=True,
    mapping=mapping,
)

"""# Split Dataset into training and validation"""

splits = windows_dataset.split(by="subject")

# If you loaded subject_ids = [2, 3]
train_set = splits["2"]  # Candice
valid_set = splits["3"]  # Drake

"""# üîÅ Create Sequence Samplers for U-Sleep
U-Sleep is a sequence-to-sequence model ‚Äî it learns to predict a sequence of sleep stage labels from a sequence of EEG windows. To prepare the data accordingly, we create sequence samplers that group consecutive 30-second windows into sequences.

In the original U-Sleep paper (Perslev et al., 2021), sequences of 35 windows (~17.5 min) were used.

We use the SequenceSampler class from Braindecode, which handles the creation of these sequences and prepares them for model training and validation.
"""

from braindecode.samplers import SequenceSampler

n_windows = 35  # Sequences of 35 consecutive windows; originally 35 in paper
n_windows_stride = 3  # Non-overlapping sequences

train_sampler = SequenceSampler(
    train_set.get_metadata(), n_windows, n_windows_stride, randomize=True
)
valid_sampler = SequenceSampler(valid_set.get_metadata(), n_windows, n_windows_stride)

# Print number of examples per class
print(len(train_sampler))
print(len(valid_sampler))

""" To adjust for sleep stage imbalances (N2 is more frequent than N1 and N3), we compute Inverse-frequency-based weights,
apply to loss function and thereby penalize the model more for misclassifying rare classes (N1, N3)"""
import numpy as np
from sklearn.utils import compute_class_weight

y_train = [train_set[idx][1][1] for idx in train_sampler]
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

"""# Create CNN model (U-Sleep Architecture)"""

import torch
from braindecode.util import set_random_seeds
from braindecode.models import USleep
cuda = torch.cuda.is_available()
device = 'cuda' if cuda else 'cpu'
if cuda:
    torch.backends.cudnn.benchmark = True
# Set random seed to be able to roughly reproduce results
# Note that with cudnn benchmark set to True, GPU indeterminism
# may still make results substantially different between runs.
# To obtain more consistent results at the cost of increased computation time,
# you can set `cudnn_benchmark=False` in `set_random_seeds`
# or remove `torch.backends.cudnn.benchmark = True`
set_random_seeds(seed=31, cuda=cuda)

n_classes = 5
classes = list(range(n_classes))
# Extract number of channels and time steps from dataset
in_chans, input_size_samples = train_set[0][0].shape
model = USleep(
    n_chans=in_chans,
    sfreq=sfreq,
    depth=12,
    with_skip_connection=True,
    n_outputs=n_classes,
    n_times=input_size_samples
)

# Send model to GPU
if cuda:
    model.cuda()

"""### üîÅ Training the U-Sleep Model

We use the `EEGClassifier` from Braindecode to train our deep learning model (U-Sleep) on Candice's sleep data and validate it on Drake's data. We apply class weights to handle imbalance and use callbacks to track **balanced accuracy** during training. The model is trained for 35 epochs using a batch size of 32 and the Adam optimizer.

‚úÖ What exactly is EEGClassifier doing?
Think of EEGClassifier as your trainer wrapper. It‚Äôs built on top of skorch, which makes PyTorch models behave like scikit-learn models.

Here‚Äôs what EEGClassifier handles:

Task	What EEGClassifier does

üíæ Loads data	Takes your train_set and valid_set

üì¶ Organizes batches	Handles batching of EEG sequences

üß† Runs forward pass	Feeds EEG windows to the U-Sleep model

üîÑ Backpropagation	Computes gradients and updates model weights

üìâ Calculates loss	Uses CrossEntropyLoss for classification

üìà Tracks metrics	Runs custom callbacks like balanced accuracy

üß™ Evaluates on validation	After each epoch using valid_sampler

üß† Predicts	Provides .predict() like sklearn models

üíª Handles device	Automatically uses GPU if available

"""

from skorch.helper import predefined_split
from skorch.callbacks import EpochScoring
from braindecode import EEGClassifier
from sklearn.metrics import balanced_accuracy_score

#Define learning settings
lr = 1e-4           # learning rate (how fast the model updates)
batch_size = 32     # number of sequences per training step
n_epochs = 35        # total training passes over the data

def balanced_accuracy_multi(model, X, y):
    y_pred = model.predict(X) #model's predicted sleep stage
    return balanced_accuracy_score(y.flatten(), y_pred.flatten())

#Metric callbacks - to track accuracy after each epoch
train_bal_acc = EpochScoring(
    scoring=balanced_accuracy_multi,
    on_train=True,
    name='train_bal_acc',
    lower_is_better=False,
)
valid_bal_acc = EpochScoring(
    scoring=balanced_accuracy_multi,
    on_train=False,
    name='valid_bal_acc',
    lower_is_better=False,
)
callbacks = [
    ('train_bal_acc', train_bal_acc),
    ('valid_bal_acc', valid_bal_acc)
]

clf = EEGClassifier(
    model,                           # your U-Sleep model
    criterion=torch.nn.CrossEntropyLoss,        # loss function
    criterion__weight=torch.Tensor(class_weights).to(device),  # handle class imbalance
    optimizer=torch.optim.Adam,     # optimization method
    iterator_train__shuffle=False,  # keep sequences in order
    iterator_train__sampler=train_sampler,      # define how training sequences are picked
    iterator_valid__sampler=valid_sampler,      # same for validation
    train_split=predefined_split(valid_set),    # use our custom validation set
    optimizer__lr=lr,
    batch_size=batch_size,
    callbacks=callbacks,
    device=device,     # run on GPU or CPU
    classes=classes    # label values: [0, 1, 2, 3, 4]
)

# Deactivate the default valid_acc callback:
clf.set_params(callbacks__valid_acc=None)

# Model training for a specified number of epochs. `y` is None as it is already
# supplied in the dataset.
clf.fit(train_set, y=None, epochs=n_epochs)

"""### üìà Training Curves

We visualize how the U-Sleep model's performance changes over training. The first plot shows loss, and the second plot shows balanced accuracy . This helps us monitor overfitting, convergence, and overall learning progress.
"""

import matplotlib.pyplot as plt
import pandas as pd

# Extract loss and balanced accuracy values for plotting from history object
df = pd.DataFrame(clf.history.to_list())
df.index.name = "Epoch"
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 7), sharex=True)
df[['train_loss', 'valid_loss']].plot(color=['r', 'b'], ax=ax1)
df[['train_bal_acc', 'valid_bal_acc']].plot(color=['r', 'b'], ax=ax2)
ax1.set_ylabel('Loss')
ax2.set_ylabel('Balanced accuracy')
ax1.legend(['Train', 'Valid'])
ax2.legend(['Train', 'Valid'])
fig.tight_layout()
plt.show()

"""# Model Evaluation
Confusion matrix and classification report
"""

from braindecode.visualization import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, classification_report

y_true = np.array([valid_set[i][1] for i in valid_sampler])
y_pred = clf.predict(valid_set)

confusion_mat = confusion_matrix(y_true.flatten(), y_pred.flatten())

plot_confusion_matrix(confusion_mat=confusion_mat,
                      class_names=['Wake', 'N1', 'N2', 'N3', 'REM'])

print(classification_report(y_true.flatten(), y_pred.flatten()))

"""## üß† Evaluation Summary (Braindecode + U-Sleep CNN)
After training the U-Sleep CNN model on Candice‚Äôs data for 35 epochs using 35-window sequences, we evaluated it on Drake‚Äôs sleep EEG data. The model was trained using EEGClassifier from Braindecode, with class balancing and robust spectral preprocessing.

### üîç Key Observations:
Stage 2 (N2) showed the strongest precision (0.90), meaning the model was highly confident when predicting this stage. However, recall (0.51) was moderate, suggesting it missed some true N2 segments.

Stage 1 (N1) had better recall (0.55) than precision (0.18), indicating the model often over-predicted this class.

Stage 3/4 (N3) reached a recall of 0.39 ‚Äî an improvement over the MNE model ‚Äî but precision remained low (0.22).

REM sleep (R) had moderate performance, with a balance between precision (0.41) and recall (0.28).

Wake (W) detection improved over previous models, with a decent precision (0.30) and recall (0.44).

### Final Accuracy: **44%**
Considering the model was trained from scratch and evaluated on unseen data, this result is a promising baseline for deep learning-based sleep stage classification. Compared to the MNE approach, U-Sleep learned more temporal context using sequence input and convolutional layers.

### Next Steps:
- Improve class separation (especially for N1 and REM) by fine-tuning model depth or using longer training.

- Experiment with Focal Loss or label smoothing to help the model focus on harder-to-classify stages.

- Visualize model predictions alongside raw EEG to understand errors better.

- Add regularization (e.g., dropout layers) to improve generalization.

- Train on more subjects (e.g., add Alice and Bob) to reduce overfitting to Candice's EEG patterns.


"""